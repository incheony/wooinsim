dueling이란 말은 싸우다라는 뜻인데
딱히 싸우지 않아서 제목이 왜 dueling인지 모르겠음

네트워크 아키텍처에 대한 내용임,
네트워크를 다른 구조로 제안함. 논문 첫 페이지 우측 사진이 바로 그것임.
맨 끝 부분이 다름.

하나는 state value function을 다른 하나는 state-dependent action advantage라는 function으로 나타냄.

위에 그림은 cnn 통과해서 stride max pooling 등을 통과하고 이떄 
깊이는 filter의 개수에 따라 달라짐.
그리고 feature를 추출해서 fully connected layer로 들어감.
여기서는 마지막 fully connected layer의 유닛은 1024개를 사용함

근데 아래 그림에서 제안하는 거는..
1024를 512 512로 줄여서 하나로 만들고
또 이거는 액션의 갯수에 따라 바뀜.. 밑에서 마지막 두개의 빨간거.
출력으로 Q-value이고.. 
마찬가지로 dueling nn에도 Q-value가 출력임
근데 이를 더함. (그림에서 초록색 부분)
하나짜리는 V, 밑에는 A

즉 a랑 v로 쪼개서 a+v를 통해 q값이 나오게끔 함

fully-connected layer를 반으로 쪼개서 다음으로 하나의 layer를 통과해서 더해서 q값이 나오게하는..
nn이 더 깊어지지 않았나 싶음. 그래서 성능이 더 좋아진 것 같음. (3:12)

그럼 A는 무엇인가?
수식(3) 부분,,
원래 있던 함수긴 한데 Q-V임.. (3:45)
각각의 값에서 평균값을 빼는 거..
-------
|   |   |
-------
액션에 대한 판이 있으면.. 이에 대한 평균을 취하면 이게 v고..
Q-V는 .. 각각의 값에서 평균을 뺀다는 의미.

Q가 S,A에 대한 함수이기 때문에 A도 S,A에 대한 함수임.
DQN, DOUBLE DQN, PER
여기선 DDQN+DUELING NN을 제안함


A는.. A = Q-V임 (5:00)
근데 우리가 알고싶은 건Q이기 때문에
Q = A+V(논문에 해당 식이 나옴)
여기서 저자는 양변에 E를 취하면,,
V의 정의가 Q의 Expected value이므로
양변에 Expected 를 취하면 A의 Expectation은 0이다. 

E[A+V]=E[Q]=V (5:40)
여기서 Expectation은 policy에 대한expectation이기 때문에 
그 policy의 distribution을 따라서 action에 대한 그런 mean값을 의미하므로
V가 그대로 튀어나옴. 왜냐하면 action이 아니기 때문에. 상수이므로.
그래서 
E[A+V] = E[A] + V = E[Q] = V
그러면 결국에 E[A] = 0임...위의 식을 따르면.. (6:105)

이거랑..
prior information이라고..
advantaged function이란 것을 사용할건데, action에 대한 평균이 0이라는
prior information이 있는 상태에서 이를 네트워크 구성할 때 이용하는 것임.

Moreover, (6:40)
deterministic policy!
Q값이 가장 커지도록 하는 액션을 고르는 그런 determinstic policy라면..
즉 .. policy인데 그냥 하나만 고르는 policy
근데 policy에 대하여 액션을 샘플을 해서 평균을 취할텐데 
그렇다면 그 값만 계속 추구?추출?를 하니까
평균값은 그냥 이거 max값이 바로 그 평균값이 됨..
(7:10)


판대기가 있고
----------
| 1   /     |
| /    `1    |
-----------
대충 동서남북 그림;;;

    3
6      4
    5
determinstic policy는 그냥 하나만 고르는거고
왼쪽 6을 고르느 policy라면
우리가 평균을 구할때
적분기호xp(x)dx
[시그마 부분: 아이패드필기]

근데xi가 p를 따름.
p를 따라서 많이 뽑았을 때의 평균값인데,
6만 뽑는 거야...
6을 3번 뽑으면
6+6+6 / 3 = 평균값은 6
그래서 6 자체가 평균값이 됨
그래서 determinstic policy라면.. 
argmax(a)를 q값에 집어넣는것은 그게 곧 평균이니까 
그 정의에 의해서 평균은 V고, V랑a*를 넣은 Q는 같다! Q(s,a*)=V(s)

왜냐하면 v는 평균값인데.. 6만 계속 뽑는 policy니까 6이 결국 평균인v가 된다.

그러면 advantaged에다 a* 를 넣으면 0이어야 한다. 즉 A(s,a*)=0 (8:35)
왜? 같은 방식임.. a+v = q이고....
q a* = v여야하니까 a = 0

여기서 제안하는건 첫번째 또는 두번째 암거나 쓰고..
코드들을 보면 주로 1번을 쓰는 것 같음.. (9:00)

앞부분은 dqn cnn 같은 거를 쓸텐데
뒷부분은 알파, 베타로 사용함

처음 그림에서..
쪼개기 전까지는 모두 동일
근데 쪼개면서
weight를 다르게 씀

위에가 알파인지 베타인지는 생각안남.. 암튼 다르게 한다는 것!
수식보니 A가 알파, V가 베타고 이를 더해서 Q를 만듦~!(10:00)

(7) 수식은 1번
prior information

(8) 수식은 2번
v+a를 할건데! v+a를 통해서 q를 구할건데
a는 여기서는 알파라고 표현된 a라서 완벽한 a의 정의를 따르지 못하는데 
a의 특성상 max가 되는 action을 집어넣었을때 0이 되어야하기 때문에
식을 보면 max a를 집어넣음
괄호안이 모두 a임.

근데 여기 속에 있는 a는 알파로 파라미터라이즈한 (미완성된) a이므로 
만들어가는 과정이라서
이놈이 진짜 advantage function이기 위한 어떤 property를 따르도록 강제하는 것.
근데 마치 위에서 max a는 0이 되어야 하므로
여기다가 max a를 집어넣으면,
A(a*) - maxaA = 0

즉 전체 A가 0이 되도록..
A(a*) - maxaA = 0

(9) 식도 마찬가지. 얘는 평균. (11:35)
다른 action에 대한 값도 update되는 효과
mean = 0이어야 함.

advantage function의 평균이 0이 되어야 하는.
a': (discrete action)에 대하여 각각을 다 집어넣고 다 더해서
절댓값A: 액션의 갯수를 의미함
액션의 갯수를 a값에 다 집어 넣어서 더한 다음에 나누면 평균.
그거를 내가 키우고 있는 A에서 뺄셈함.
그러면 괄호 전체에 대한 평균이 0. 위의 식과 마찬가지.
그러면 첫번째 property를 만족하는 a가 됨.

8 or 9 르 ㄹ따르도록 네트워크를 만듦

((여기서 의문점은 저게 평균이라면 uniform random policy여야 하지 않나..))





처음 그림에서 +라고 했는데 단순하게 더하는 게 아님.
이는 property를 만족하도록 max a 또는 mean a를 빼는 것.
그러므로써 q를 만드는.

(9)수식 아래부분 설명, (13:10)
이는 기존의 nn과 다름.
nn은 값이 들어오면 layer는 weighted sum해서 activation을 통과하는 거였는데
뉴럴넷은 아이패드 (13:20)

여기서는 마치 softmax처럼 평균값을 빼주면서 
다른 action에 대한 값도 같이 업데이트되는 효과를 가질 수 있음.
(13:35)
무슨말이냐면..
기존에는 action에 대한q값만 있었다
예를 들어
0-----Q(오) ....1
0-----Q(왼)
0-----Q(위)
이렇게 Q값들이 있으면 
1번 즉 오른쪽에 대한 샘플을 뽑았다면
그러면 그 샘플 값이랑
Q(오)를 뺄셈서 제곱해서 그게 minimized gradient?해서 반대방향으로 한스텝 갔다면
그말은 그 "오른쪽" 값에 대한 것만 집중해서 업데이트 하는 것임
즉 오른쪽값만 집중해서 보고 걔가 어떤 샘플값을 더할 수 있도록 업데이트해라.라는 의미.

(14:10)
얘는 (9) 수식에서 ( )이속에 A라는 함수가 들어있다 보니까
오른쪽에 있는 q값만 바꾸더라도
수식 맨 우측에서 
오른쪽 왼쪽 위쪽 다 더해서 나누기 때문에 오른쪽에 대해서는 업데이트 하더라도
그 전 layer에서는 이 행위를 할때는 이게 다 들어가기 떄문에 다른 action에 대한 값도 
꽤나 크게 업데이트 될수있다.(그이전보다)

그 이전에도 cnn weight 전체가 바뀌니까 값도 바뀌긴했을텐데
미미하게 바꼈고 (오른쪽이 바뀌면 왼쪽이)
의도하지 않은 바뀜..
(14:50)

근데 여기서는 action에 대한 평균값을 빼주는 것을 전 layer에서 해주기 때문에
그다음에 더해서 마지막 layer로 감.
암튼 그 전 layer에서 해주면서 다른 action에 대한 값도 같이 업데이트가 됨. 
이 이전에 a가 들어있기 때문에.
오른쪽 값을 업데이트 하는데 여기서 평균값을 가지고 
아까 그 예제에서 a끼리의 함수로 꼬여있음.
a의 action에 대한 값을 평균을 구해서 빼버리니까(15:20)
그게 여기 섞여져 있기 때문에
A-----0-----Q(오) 
A-----0-----Q(왼)
A-----0-----Q(위)

기존에 마지막 단에서 Q(오) 만 바뀌게 하는 거랑은 차이가 있음.

지금은 마지막 Q(오)만 바꾸더라도
이전에 A에서 다 들어있어서 그니까 A에 대한 평균값이 들어가 있어서
다른 action에 대한 값도 업데이트 되는 효과가 있음!

(15:45)
즉 그냥 q로 빼지 말고 a랑 v로 바꿔서 빼서
a에 대한 property를 만족하도록 식을 세운 다음 v랑 더해서
q값을 뽑아내자! 이런 구조임!

(16:05)
prior information이 이용됐기 때문에 이것만큼 좋은 성능이 나온 것 같기도함.
아예 기존에는 아무런 prior정보를 사용하지 않았음 
즉q값이 어떻게 되어야한다 그런게 없고
그냥 action에 대한 q값을 뱉어줘하고 네트워크를 키울 뿐이었는데
a랑 v로 쪼개서 바꾼 다음에 그거를 더할 떄 a에 대한 property 
a에 대한 prior information을 만족하도록 a를 바꾼다음에 q값을 구하기 때문에
좋은 효과가 나온것 같음.

실험 : 아타리게임 (16:45)
fully-connected
dqn hyper-paremeter사용(감마 러닝레이트 등 동일하게 씀)
즉 동일한 조건에서 비교를 함.

여기서는 ddqn위에 dueling network를 활용함.

끝

https://greentec.github.io/reinforcement-learning-third/#dueling-dqn
https://sumniya.tistory.com/19
https://bluediary8.tistory.com/9
https://taek-l.tistory.com/37
------------------------------------------------------------------------------







