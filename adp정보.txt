https://cafe.naver.com/datageeks?iframe_url=/ArticleList.nhn%3Fsearch.clubid=26910945%26search.menuid=91
https://cafe.naver.com/datageeks/1255
https://calebpro.tistory.com/553
https://ckmoong.tistory.com/category/Work/ADP?page=2
https://ckmoong.tistory.com/7?category=933194
https://gorakgarak.tistory.com/1300
https://shiningyouandme.tistory.com/7
https://didalsgur.tistory.com/70
https://ysyblog.tistory.com/114
https://didalsgur.tistory.com/25?category=750762
https://m.blog.naver.com/sinrakr/222185635683


- 회귀분석 : feature engineering / 다중공선성 확인
- 분류
- 텍스트마이닝 : 빈도수체크 / associatino rule(시간 없음 주의)


[1] 데이터 불러오기

[2] 데이터 탐색
(1) 데이터 결측치 찾기
(2) 빈값, 음수면 안되는데 음수인 값은 NA로 넣어주기
(3) 데이터 타입 변경하기
(4) 변수 분포 확인하기
값의 범위가 극단적이면 정규화
분포가 극단적이면 로그 변환
(5) 연속형 자료 이상치 핸들링
dbscan 함수
(6) 변수 별 빈도수가 유독 적으면 upsampling
(7) 선형모형 적합시 독립변수 간 상관관계 없도록 처리
다중공선성 높은 변수 제거
주성분 분석 시행

[회귀분석R]
https://rstudio-pubs-static.s3.amazonaws.com/190997_40fa09db8e344b19b14a687ea5de914b.html
[시계열]
https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html#time-series-analysis
https://ckmoong.tistory.com/category/Work/ADP
[som]
https://stackoverflow.com/questions/51500092/r-self-organized-maps-predict-new-data-with-fitted-unsupervised-som
https://m.blog.naver.com/PostView.nhn?blogId=pmw9440&logNo=221588292503&proxyReferer=https:%2F%2Fwww.google.com%2F
https://heave.tistory.com/4


https://ysyblog.tistory.com/search/adp?page=2
https://didalsgur.tistory.com/search/adp
https://0dood0.tistory.com/search/adp
https://blog.naver.com/sujebee
https://m.blog.naver.com/sinrakr/221778920182
성신여대 파이썬 프린트
파비 프린트 

[참고링크 목록]
https://blog.naver.com/sujebee/220696552386
https://docs.google.com/presentation/d/1nfjNz95C3Xb6BFjXQyWktNWXAcZqpYzOn-DDQIXW4JA/edit?usp=sharing
https://github.com/taeshahn/DataAnalPrac

15회차
https://cafe.naver.com/sqlpd/10794

군집분석
https://cafe.naver.com/sqlpd?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D21771779%2526page%3D11%2526inCafeSearch%3Dtrue%2526searchBy%3D1%2526query%3Dadp%2520%25EC%258B%25A4%25EA%25B8%25B0%2526includeAll%3D%2526exclude%3D%2526include%3D%2526exact%3D%2526searchdate%3Dall%2526media%3D0%2526sortBy%3Ddate%2526articleid%3D3504%2526referrerAllArticles%3Dtrue
군집수결정
https://cafe.naver.com/sqlpd?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D21771779%2526page%3D11%2526inCafeSearch%3Dtrue%2526searchBy%3D1%2526query%3Dadp%2520%25EC%258B%25A4%25EA%25B8%25B0%2526includeAll%3D%2526exclude%3D%2526include%3D%2526exact%3D%2526searchdate%3Dall%2526media%3D0%2526sortBy%3Ddate%2526articleid%3D3497%2526referrerAllArticles%3Dtrue
================================================

[[01회]] 2014.06.28

고객 1000여명의 동의를 얻어 1년간 아래의 정보를 수집하였다.
주어진 데이터를 분석하여 적절한 마케팅전략 및 인사이트를 도출하시오.             
________________________________________

1. 고객정보                          [1000건]
2. 인터넷접속정보           [338만여건]
3. 오픈마켓접속정보        [197만여건]
4. 상품목록                    [5만3천여건]
5. 포털사이트 검색로그 [73만8천여건]
________________________________________

■ 과제 : 고객세그멘테이션전략수립
---------------------------------------------------------------------
    세분화 집단별 예측모형의 특성 분석 및 시각화
    예측모형의 비교분석
    EDA
    상관분석
    Decision Tree
    종속변수 독립변수 선정
    변수 성격에 따른 적절한 모델링 기법
    연간구매 1개월내 구매가능성 
    구매전환율 
    검색패턴
    쇼핑단계별 이용패턴
    세분화 집단별 예측모형
    구매예측
    이탈예측
    등급변동예측
    우수고객예측 

분석결과를 토대로 적절한 마케팅 인사이트 제시 
- 텍스트 마이닝 및 Topic 분석
   전체검색어 리스트를 하나의 문서로 간주

■  간단 분위기 
응시인원 8명 아마도 필기 합격자 수와 일치하지 않나 싶음 ( ADP 필기 응시가 두반이었으니 대략 10% 필기 합격률인 듯 )
응시자 연령 30대 중반에서 40대로 보임 (남6 여2) - 연령대에 약간 놀람 
최종 합격률 예상은 시험을 주관한 측에서도 합격자가 있어야 하니 한 3~4명 정도로 예상해 봄 (1회필기+실기 최종합격자 5% 내외)


================================================

[[6회]] 데이터전처리/데이터마이닝, 통계분석, 텍스트마이닝
https://blog.naver.com/sujebee/220699299147

문제1. 데이터 전처리 및 데이터 마이닝(R로 QF(?) 알고리듬을 직접 구현하여 단계별로 만들어 보는 문제)
- 데이터
sales.csv 5천건
cust_id,prod_id,amt
1,prod01,100
1,prod02,200 
11,prod02,300
21,prod03,100 
...
(가) sales.csv를 읽어서 cust_id 및 prod_id별로 amt가 0보다 크면 1, 아니면 0으로 아래 처럼 만들 것
cust_id prod01  prod02  prod03  prod04  prod05  prod06 ...
1     1      0        1       1         0        0
      11    0       1        0       0         0        0
      21    0        0        1       1         1       0
      31    1       1        0        1        1       1
...

(나) 위의 결과를 이용하여 cust_id 별로 상호 인접정도를 파악하기위해 cust_id별로 피어슨상관계수 행열을 아래와 같은 모양으로 만들 것
    1    11     21     31...
1  1.00  0.72   0.61    0.83
11 0.72  1.00   0.75    0.88
21 0.61  0.75   1.00    0.93
31 0.83  0.88   0.93    1.00
... (가)의 결과를 그냥 cor함수에 넣으면 오류남

(다) 특정 user의 유사도가 높은 15 user 구해서 다음 형태로 만들기? (행과 열이름은 cust_id였고 각 행렬의 값은 같은 상품을 구매한 amt 였던 것으로 추정됨)
   1  11 21 31...
1 1200 100 200 400
11 100 2300 500 900
21 200 500 1500 800
31 400 900 800 1000
...

(라) 특정 user 당 5개씩 추천 아이템 생성하기? (잘 기억 안남)
prod01,prod03 ,prod06....


2. 통계분석
- 데이터 : risk_proj.txt (성별,인종별,나이,활동성,risk)

(가) 성별에 따라 risk가 차이가 있는지 분석할 것
나이등 다른 변수와 교호작용 있는지 알아볼 것(가정사항등 정의)

(나) 인종별 risk가 차이가 있는지  분석
나이등 다른 변수와 교호작용 있는지 알아볼 것(가정사항등 정의)

(다) 인종및 성별에 따라 risk가 차이가 있는지 분석. 교호착용 분석


3. 텍스트 마이닝
- 데이터 : location.txt  (UTF-8, 헤더 없음)
  가평,ncn
  가야,ncn
  남이섬,ncn
   ....

- 데이터 : blog.txt  ( TAB문자로 구분됨)
  DATE      TITLE        CONTENT
  20150101 제목        봄관련 내용....
  ....

(가) 자료읽기
   (ㄱ) location.txt를 읽어서 사용자사전에 등록하기
   (ㄴ) blog.txt를 다음 형식으로 읽을 것
     DATE :  numeric
     TITLE : character
     CONTENT : character

(나) blog.txt에서 봄여행,벚꽃축제,봄나들이 등 봄과 관련된 문서만 추출하기
   (가)에서 읽은 사용자 사전에 들어있는 지명이 들어있는 문서만 추출

(다) 위에서 추출된 문서에 대해 명사추출 및 출현 빈도 10위 추출

(라) 봄과 관련된 지명 출현 빈도 10위까지 추출하여 시각화

[답안]
(가)번은 생각보다 간단합니다.
reshape2::dcast()함수를 이용하면 됩니다.
dcast(data,ids~ values,FUNC) 함수는 values값을 열(칼럼)으로 만들어 펼쳐주는 기능을 합니다.
원래 melt로 만들어진 DF를 원상복귀할 때 주로 사용하는 것이지만
반드시 melt가 선행되어야 하는 것은 아닙니다.
값을 속성으로 만들고 싶을 때 쓰면 편리합니다.
즉, "prod01","prod02"과 같은 값들을 prod01,prod02이라는 칼럼으로 만들어 주기 때문에
우리가 원하는 모양으로 만드는데 가장 적합한 함수 입니다.


#먼저 테스트 데이터를 만들겠습니다.
실제 문제는 sales.csv로 주어졌지만 파일이 없으므로 최대한 유사하게 만들어 보았습니다.

#cust_id: 1~991 사이 숫자 5천개(1,11,21,31...991)
set.seed(1)
cust_id<-sample(seq(1,1000,10),5000,replace=TRUE)
NROW(unique(cust_id))  #unique한 cust_id는 100개

#amt : 0~50000 사이의 숫자 5000개를 뽑고 sparse matrix를 만들기 위해 4500개 숫자는 0으로 만듬
amt<-round(runif(5000,0,50000),-1)
inx<-sample(1:5000,size=4500) #0으로 만들 인덱스 번호
amt[inx]<-0  #4500개 값을 강제로 0으로 만듬

#prod01,prod02,...,prod10
prod_id<-sample(sprintf("prod%02d",round((runif(100,1,10)),0)),5000,replace=TRUE)
head(prod_id)
sales<-data.frame(cust_id=cust_id,prod_id=prod_id,amt=amt)
head(subset(sales,amt>0))
##---------------------------------------------------------------------------------------
##      cust_id prod_id   amt
##1      261  prod03  1770
##21     931  prod04 13900
##39     721  prod04 21520
##52     861  prod07 17490
##74     331  prod05 43120
##75     471  prod09  4870
##---------------------------------------------------------------------------------------

##[가] 희소 매트릭스 처럼 prod_id별로  금액이 있으면 1,없으면  0으로 만들기
sales.cast<-dcast(sales,cust_id ~ prod_id,value.var="amt"
                  , function(x) {
                    s<-sum(x)
                    ifelse(s>0,1,0)
                    })
head(sales.cast,10)
위 함수의 뜻은 cust_id를 고정하고 prod_id를 칼럼으로 만들면서
amt값에 대해 지정된 함수를 호출하라는 뜻입니다.
함수의 내용은 먼저 cust_id/prod_id별 amt합계를 구한다음 이 값이 0보다 크면 1을 아니면 0으로 설정하는 것입니다.
결과적으로 단 한줄로 원하는 답을 얻을 수 있습니다.(여기까지는 쉽게 풀었습니다....ㅠㅠ)
##---------------------------------------------------------------------------------------
## (1-가.답)
##   cust_id prod01 prod02 prod03 prod04 prod05 prod06 prod07 prod08 prod09 prod10
##1        1      1      1      1      0      1      0      1      1      0      0
##2       11      0      1      1      1      0      0      0      0      0      0
##3       21      0      0      0      1      1      0      1      1      0      0
##4       31      1      1      1      0      1      1      1      1      0      1
##5       41      0      0      0      1      1      1      1      1      1      0
##6       51      0      0      0      0      0      1      1      0      1      0
##7       61      0      0      1      0      0      1      0      0      0      0
##8       71      0      1      1      1      0      0      1      0      1      0
##9       81      1      0      1      0      1      0      0      0      1      0
##10      91      0      1      0      0      1      0      0      0      1      0
##---------------------------------------------------------------------------------------



#[나] cust_id간 피어슨 상관계수 행렬을 만들기
#그냥 cor함수를 호출하면
# cust_id간의 상관계수가 아닌 prod_id간의 상관계수가 나옵니다.
round(cor(sales.cast[,-1],method="pearson"),3)
##---------------------------------------------------------------------------------------
##       prod01 prod02 prod03 prod04 prod05 prod06 prod07 prod08 prod09 prod10
##prod01  1.000 -0.171  0.113 -0.032  0.182  0.016 -0.010  0.078 -0.089  0.245
##prod02 -0.171  1.000 -0.093  0.015 -0.021  0.035  0.108 -0.014  0.018  0.003
##prod03  0.113 -0.093  1.000 -0.016 -0.040 -0.013  0.063 -0.030  0.014  0.136
##prod04 -0.032  0.015 -0.016  1.000 -0.141 -0.100  0.034  0.075  0.157 -0.044
##prod05  0.182 -0.021 -0.040 -0.141  1.000 -0.224  0.060  0.083 -0.042  0.000
##prod06  0.016  0.035 -0.013 -0.100 -0.224  1.000  0.111 -0.032  0.003  0.129
##prod07 -0.010  0.108  0.063  0.034  0.060  0.111  1.000  0.254 -0.041  0.165
##prod08  0.078 -0.014 -0.030  0.075  0.083 -0.032  0.254  1.000 -0.055  0.163
##prod09 -0.089  0.018  0.014  0.157 -0.042  0.003 -0.041 -0.055  1.000 -0.134
##prod10  0.245  0.003  0.136 -0.044  0.000  0.129  0.165  0.163 -0.134  1.000
##---------------------------------------------------------------------------------------

원하는 모양이 나오지 않습니다.
아무리 cor함수를 이리 저리 호출해봐도 마찬가지 입니다.
ㅂ
P.S. 1-(나) 피어슨상관계수 행렬을 구할때 간단한 방법이 있었습니다.
해현용님의 조언에 따라 아래와 같이 하니깐 한번에 해결이 되었습니다.
sales.mat<-as.matrix(sales.cast[,-1]) 
cor(t(sales.mat))

행렬을 t()함수를 이용하여 행과 열을 바꾼후(cust_id가 행이 아니라 열쪽에 위치하도록 있도록 변환)
cor()을 호출하면 되는 거였습니다...ㅠㅠ
cor()함수는 <열방향>끼리의 상관계수를 구해준다는 것을 오늘 처음 알았습니다.
이렇게 무슨 함수를 써야 하는지는 대충 알지만 정확한 사용법을 몰라서 헤맨 케이스 였습니다...ㅠㅠ
아직 공부가 멀었다는 생각이 듭니다.
================================================

[[08회]] 2017.04.30

[1.통계 분석]
1.폐활량(Fev) 예측 문제 (나이, 키, 성별, 흡연 유무) 
-EDA 및 상관관계 분석
-적절한 회귀모형 선택
-회귀모형 해석 (변수 별 증가에 따른)
-평균 키, 나이(여자, 흡연자) 일 경우 폐활량 예측

2.통계 문석 문제 - 12회 기출
-변수 시각화(변수간 상관관계, 변수별 이상치 파악)
-회귀모형 적합과 유의성 검정
-회귀 계수에 대한 standard error가 가지는 의미
-회귀분석에서 잔차 분석 및 시각화
-회귀분석에서 영향력 관측치와 그 영향 분석

[2. 데이터 마이닝]
1.백화점 사용패턴 분석 

Date (0000-00-00 00:00 형태)  
customer id  
product (한글 백화점 상품명)  
price (가격)

-파생 변수 생성 및 EDA
-군집분석 및 적절한 집단 갯수 설정
-세그멘테이션 별 의미 파알 (집단 라벨링)
-세그멘테이션 별 마케팅 인사이트 도출

2.고객 구매데이터 분석

-고객 구매데이터(transaction data)에서 이후 1개월 간 실제 고객이 구매할 것으로 예정되는 지점 추천
-5개 지점 추천 후 하나라도 맞으면 맞는 것으로 간주 적중률 66.7% 이상인 경우만 채점, 그 이하인 경우 0점으로 간주

3.타이타닉 데이터 분석
상세 없음

[3. 텍스트 마이닝]
1.TV Program Buz 분석 

-Tvprogram 파일에 있는 단어들을 사전에 추가하기
-Tvprogrambuz 파일을 읽고 데이터 전처리
-월별/프로그램 별 나온 단어 분석
-월별 프로그램 비율 확인

2.뉴스 기사 분석 - 12회 기출

-뉴스기사 로딩 및 제공된 긍정/부정 어휘를 통한 감성분석
-위에서 구한 긍정부정 score를 통해 N개의 그룹으로 클러스터링

3.영화 리뷰 분석 - 11회 기출
상세 없음

================================================

[[11회]] 2018.10.
https://0dood0.tistory.com/35?category=1012731
  
1. 통계학 기반 분석
2. Text Mining을 적용한 분석
3. Data mining(ML) 학습을 통한 결과 도출 


1. 통계학 기반 분석 (40점)
- 각 설명변수들과 출산률(종속변수)의 관계를 회귀분석으로 정의 및 결과를 해석하는 문제  

2. Text mining (20점)
- 영화평 Data를 전처리 후, '형용사'를 추출 하여 감성 분석 하는 문제
사실 R을 활용한 Text Mining 대부분의 예시는 '명사'를 추출하는 형태로 되어 있고, 명사 추출은 extractNoun 함수로 쉽게 수행을 할 수 있다.
그런데 형용사 같은 경우는.... SimplePos22 함수를 써야하는데 해당 부분을 제대로 숙지하지 못하고 가서, 결국 for문을 만들다 시간이 부족해 실패를 해버린 것이다.

3. Data Mining/ML (40점)
- (R을 공부하는 많은 사람들이 익숙한) 타이타닉 생존자 Data를 Data mining 학습하여, 생존여부 예측을 하는 문제 
- 금번회차의 경우 분석 과정은 상관없고,  오직 제공된 Test Data의 정답만을 제출해서, 예측한 정답의 적중률이 얼마나 높은지로 채점을 함

================================================

[[14회]]
출처: https://didalsgur.tistory.com/32 [Took-Took]

기계 학습을 이용하여 집 가격 예측 및 검증
다중 로지스틱 회귀 분석 및 confusion matrix 해석



================================================

[[15회]]
https://0dood0.tistory.com/64?category=1012731

1번은 무슨 제철회사 Iot data 같은 것들을 제공해 주고,
- EDA(탐색적 데이터 분석) 해보는 거
- 독립변수 선별 (feature engineering)
- 종속변수(y)를 이항으로 바꾸고 로지스틱 회귀 분석하기
- 종속변수(y) 다항인 상태에서 SVM 포함하여 3가지 알고리즘으로 돌려보고 평가
- 위에서 만든 모델 중 하나 적합한 모형 찾아서 군집분석 실시하고 군집분석을 반영하여 F1 score값을 통해 
모델이 나아지는지 확인과 같은 것을 진행하는 것이다.

1번문제도 기존 11회 실기의 머신러닝 문제와 비교해 나름 난이도가 낮은 편은 아니었지만, 
그나마 손을 댈 수는 있었는데, 솔직히 군집분석을 기존 머신러닝 결과에 반영하라는게 무슨 의미인지 몰라서,
좀 벙찌기도 했었다.
(나중에 카페 글을 보니 해당 군집을 또 독립변수로 놓고 학습을 하면 모델의 질이 높아질 수 있다. 뭐 그런 뜻인 것 같다. 솔직히 이와 같은 분석 방식을 난 전혀 모르고 있었다)

2번은.. 전력사용량 관련 data를 제공해주었는데
그 망할 timestamp라는 무슨 열 몇자리 숫자로 구성된 데이터가 나와서,
이걸 어떻게 변환하지? 머리가 새하얗게 변하면서 그냥 망해버렸다.
가장 기본이 되는 시계열 column (3개의 data의 key가 되어야 하는 column이었음) 을 변환을 못하니 뭐 그냥 문제 자체에 손을 못대고 넋을 놓게되어 버렸다.

솔직히 2번문제 접하고 나서,  와... 이건 내가 이 Data형을 실무에서 경험하지 못했다면, 
아무리 공부를 많이 했어도, 책을 3~4권 더 봤어도 이걸 풀 수 있었을까? 라는 생각이 들면서
마음속 깊이, 내 자질을 의심했다. 

이것도 알고보니 python 함수하나로 간단히 해결되는 문제인데,  뭐 내가 이런걸 봤어야지 .ㅠ
(datatime package안에 fromtimestamp 뭐이런 함수가 있음... 열라 간단...)

사실 내가 참고한 python 데이터 분석 관련 책, R 책에는 정말 조금이라도 비슷한 유형이 하나도 나오지 않아.. 
정말 당황스러운 시간이었다..




1.제조 생산 데이터 분석
1)데이터 탐색 : EDA
2)데이터 전처리 : 변수 선택(VIF), 파생변수 생성, 데이터 분할(train/validation/test(20%))
3)로지스틱 분석 : 분류1 을 판단 하는 모델 생성 (종속변수는 총 7개 분류, 분류1 외의 값은 0으로 치환), confusion matrix 해석
4)로지스틱 0분석 외 3개 이상 분류 모델 생성 및 결과 해석 : SVM 필수 포함, Precision/Sensitivity 결과 출력
5)위 모델 중 최고 모델을 선택하여 최적의 군집 개수를 선택하고 클러스터링 수행 : F-1 Score 출력

2.데이터 처리 및 통계 분석
-timestamp 처리 / date 기준 데이터 병합
-hh:mm, A/B/C/D/E, 전력 사용량 데이터
-yyyymmdd, 평균 기온
1)아래 형태의 데이터 생성 : 3개 데이터를 date기준으로 병합 필요
-> yyyy-mm / A / B / C / D / E / 사용량
2)요일 변수 생성 및 A/B/C/D/E별 평균 사용량 출력, 그래프 출력
3)요일간 사용량 분석을 수행하고 가장 차이가 있는 요일 도출
4)(문제가 생각이 안나네요.. ㅠㅠ)


================================================
[[17회]] 2020.06.21
https://bigdata-analyst.tistory.com/m/34?category=825660
https://0dood0.tistory.com/150
https://statinknu.tistory.com/19
https://didalsgur.tistory.com/70

Q1) ML
Housing data (not california housing but temp)
데이터 컬림이 약간 다른 것 빼고는 비슷함.
특히 log1p로 정규화시키는 것도 비슷.

1.1) EDA, PreProcessing (5점)


1. 주택가격 예측을 위한 ML 모델 생성/평가
- 여러 설명변수들로 Price (수치형 종속변수) 를 예측하는 모델을 만드는 문제였음
- 집값에 영향을 미칠만 항목 (방개수, 부엌, 모델링여부 등)이 독립변수로 나오고 집값이 종속변수로 나옴
- 시각화, 전처리, 회귀 모델 평가, 규제, 앙상블 , +a 등 3개의 모델을 Training 시켜 결과를 보는 문제였음


Q2) COVID-19 코로나 바이러스 시각화/시계열/비시계열 예측 모델 만드는 문제
- 국가별, 일별, 인구수, 확진자수, 사망자수, 완치자수, 검사자수   를 데이터로 줌
- 시각화, 전처리 등은 모델 생성하는게 목표기 때문에 기본으로 깔고 감
- 인구대비 확진자수를 도출해서 (파생 컬럼) Top 5인 국가를 추출 후 시각화 하는 문제 나옴
- 분석가의 역량을 보고자 낸 것 같은데,  확진자수, 사망자수, 인구수, 검사자수, 완치자수 등 변수를 활용하여
'위험지수'라는 파생컬럼을 만들어 보라고 함. 그리고 왜 그렇게 위험지수를 도출했는지 설명,
- Top10 위험지수 국가 시각화
- 시계열 분석해서 '한국' 국가의 확진자수를 예측하는 문제가 나옴
- 시계열 모델뿐만아니라 비시계열 모델로도 모델을 별도 생성 문제 나옴

2.1) 전체 인구대비 코로나 환자가 높은 국가 top10을 뽑아서 시각화.
- 일별 확진자수, 일별 완치자수 코딩 필요
- 데이터셋은 일별 누적확진자 데이터가 나와있음

2.2) 코로나 위험지수를 직접 만들고 그 위험지수에 대한 설명을 적고
위험지수가 높은 국가들 10개를 선정해서 시각화

2.3) 한국의 코로나 확진자 예측 (선형 시계열모델 + 비선형시계열 모델 2개)

Q3) 통계분석 (50점)
데이터셋 : 설문조사 데이터. 이 설문조사 데이터는 기본적으로 묶을 그룹이 2개이다.
A~S까지의 그룹이 각각 같은 설문조사를 하여 
1-1,1-2,1-3...5-1,5-2 인 설문지를 푼 것이다. 
이 때 중간에 반대 문항이 들어가 있다. 
예를 들어 1-1 문제가 나는 시간약속을 잘 지킨다.라는 문제라면 
1-3의 문제는 나는 시간약속을 잘 지키지 않는다. 라는 문제로 구성되어있다.

설문조사 데이터가 문제로 나옴
데이터는 대략
조사 번호, 그룹, 문항1-1 , 1-2, 1-3 .....     6-8  
이런 컬럼을 가진 테이블 데이터를 주고,  문항 컬럼에 들어가는 값들은 만족도 지수를 1~5점척도로 조사를 한 값이었음
1-1~ 1-x 는 항목 1,
2-1 ~ 2-x 는 항목 2
이런식으로 항목 영역이 규정된다고 전제를 함
그리고 역항목이라고 해서 1-1번의 역항목은  1-3 이고..
(설문조사시 동일내용에 대해 서로 긍정/부정 상반되는 문항을 제시해서 신뢰도를 올리기 위한 그런 항목으로 이해를 했다.)

그리고 특정 항목의 
1. 그룹별, 영역별 기술통계량 (평균, 표준편차, 첨도, 왜도)
각 영역별 그룹별 만족도 추세가 어떤지? 탐색
2. 그리고 요인분석
3. 신뢰성 지수라는 걸 구하는 식을 주고 (대략 각 영역별 correlation의 평균과 개수가 필요한 식이었음)
그걸 구하는 문제

3-1) 그룹별 통계치
3-2) 탐색적 요인분석을 표로 만들기
3-3) 기억안남


1. Housing Data(집값 예측)
1-1) EDA 및 데이터 전처리 (시각화 및 통계량 제시)
1-2) Train Valid Test set으로 분할 및 시각화 제시
1-3) 2차 교호작용항 까지 고려한 회귀분석 수행 및 변수 선택 과정 제시
1-4) 벌점, 앙상블을 포함하여 모형에 적합한 기계하습 모델 3가지 (MSE, MAPE, R2 제시)

2. Corona Data(시계열)
2-1) 인구대비 코로나 확진자 비율이 가장 높은 국가 5개 제시하고 일일확진자, 누적확진자, 일일 사망자, 누적 사망자 추이를 각각 1장씩의 시각화 그래프로 시각화(차분을 이용함)
2-2)
2-3) 코로나 위험지수를 개발하고 위험지수가 높은 국가 10개를 추려내서 막대그래프로 시각화하기
2-4) 시계열 모델링 및 비선형 모델링

3. Survey Data
분석 전, 역코딩을 반영해야함.
3-1) 항목별 그룹별 만족도 응답의 평균, 표준편차, 왜도, 첨도를 구하라.
(이렇게보면 별거 아닌거 같지만 실제 데이터를 보면 말이 엄청 애매한 문제입니다)
3-2) 응답항목별 차이가 있는지 분석
(아마 Anova Table을 요구하는 것 같습니다)

3-3) 탐색적 요인분석 수행(FactorAnalysis)
3-4) 신뢰성 지수를 개발 하는 문제 항목별 신뢰성 지수를 구하라.


1. 예측 분석에 대한 문제가 나왔습니다.
 - 캘리포니아 에 관련 hosing data 였습니다. (boston housing 과 유사합니다. ) 
  위도, 경도를 포함한 세대수 등.. 일반적인 데이터이며, NA가 몇 개 있었습니다.

 (1) 기초 통계량 구하기 (배점 5점) 
 (2) 상관도 및 histogram 을 모든 변수에 대해서 그리는 것입니다. (배점 15점)
   + 각 수치들에 대한 설명 
 (3) 변수중.. median hosing value ? 인가.. 그 값을 예측하라는 문제로 RMSE와 같은 지표와 함께 
  결측치 처리도 필요하면 하라고 해서 구하는 문제입니다. (배점 30점) 

2. 4개 변수 + 목적 변수 + seq_no 총 6개에 데이터를 주어주고 4개의 변수를 PCA를 통한 분석과 
그냥 4개변수를 통해 분석을 할 때 2가지를 해보는 문제입니다.

 (1) 파생변수 생성 - 그냥 Z-정규화하는 문제입니다. / 그 후 최소, 최대를 구하라고했습니다. (10점) 
 (2) 4개의 변수를 주성분분석하는 건데, eigen values를 2제 2성분까지 구하라는 문제입니다.(10점) 
 (3) PCA1 / PCA2 성분에 대한 산점도 / value1 ~ 4에 대한 산점도 문제입니다. (각 5점) 
 (4) 목적변수가 A, B, C인데 로지스틱회귀분석을통해 Confusion Matrix 및 정확도 / F1 Score / Recall / Accuracy 등을
  구하는 문제였습니다. PCA를 통한 2개 변수 & 원래 변수 4개를 통해 분석하는 경우 (각 10점) 



기계학습 문제 (집값 예측) (30점)

1.1 EDA
1.2 모델 생성
ㄴ 1.2.1 데이터 분할
ㄴ 1.2.2 교호작용을 고려한 다중 선형 회귀 수행
ㄴ 1.2.3 3가지 분류 모델 생성 및 비교, 좋은 모델 선택

시각화 및 시계열 분석 (코로나 데이터) (20점)

2.1 전체 인구대비 누적 사망률이 가장 높은 5개 국가 추출 후, 국가별 일일확진자, 누적확진자, 일일사망자, 누적사망자 시계열 그래프 출력
2.2 위험지수 생성 및 해석
2.3 시계열 분석 및 예측 모델 생성

통계 분석 (설문데이터 분석) -> (사전에 역문항들에 대한 처리 필요)

3.1 그룹별 평균, 표준편차, 왜도, 첨도 산출
3.2, 3.3, 3.4 (문제가 기억이 나질 않네요;)
================================================

[[18회]] 2020.09.19
https://t0-0t.tistory.com/16
https://ckmoong.tistory.com/7?category=933194


크게 3문제

-ARIMA 50점 
차분하는 방법, ACF와 PACF 그래프 보는 방법, 
AR 차수와 MA 차수 정하는 방법 등을 조사. ARIMA

-SOM

1. 기계학습 : 고객 등급(1부터 5까지) 분류 예측 모형
- EDA 및 결측치 처리를 포함하여 데이터 전처리
- 파생변수 3개를 생성하고 생성한 근거를 시각화나 통계량으로 제시
- 데이터를 train_test로 나누고 train에 대해 som을 이용해 군집분석을 실시하고 최적화 수행후 confusion matrix를 그리시오
- 랜덤포레스트, 인공신경망(이부분은 기억이 잘 안나네요ㅜㅜ) 을 포함해 4개의 분류 예측 모형을 만들고 각각의 성능을 roc_auc, F1_score로 비교하시오 
- 앞에서 정한 모델에 추가로 성능을 높이시오

2. 영어 텍스트
- 영어 문장을 의미없는 단어를 없애고 형태소 분석을 하시오
- 단어 빈도를 시각화하시오 (원래는 워드클라우드도 하는 문제인데 시험 도중에 이부분은 빼주었습니다)

3. 통계분석 : 3~4년치 매출데이터(어떤 데이터인지 가물가물합니다)
- 시계열의 정상성을 만족시키시오 
- 모형을 3개 이상 만들어 비교하시오
- 앞에서 정한 모델을 진단(통계적인 진단을 요구했습니다)
- 예측의 정확도를 나타내시오

 - 18회 기출 생각나는 대로-
1) 기계학습 - SOM
데이터파일명 sales.csv
컬럼 : id(고객아이디), ....(기억안남) ,duration, count, amount
1.1) 데이터 전처리, 사유, 시각화
duration, count, amount에 결측치 존재(약 1200건)
amount에는 -존재
1.2) SOM 돌리기
1.3) 나온 모델로 예측

2) 텍스트마이닝
빈출명사를 bar chart로 나타내고, 잘보이도록 시각화
(데이터는 전부 영문이었음)

3) 통계분석 - ARIMA
데이터는 월별교통사고건수.csv
YR_MO : YYYY-MM으로 된 데이터
EVN_CNT : 교통사고 수
3.1) 주기별 평균과 분산의 정상성 검증, 방법, 방법을 선택한 이유, 가능하면 시각화
- 계절로 주기를 만들어 줬었다....
3.2) ARIMA 돌리기
3.3) 해석(OOOO표를 통해서 - 잘 기억 안남)

모든 문항마다 필요하다면 시각화를 하시오.

1. 고객 등급 예측모형
1) EDA & 결측값 채우기
2-1) 파생변수 3개 생성 & 이유 작성
2-2) Train-Test 분할(7:3) / SOM 군집분석 / 정오분류표
2-3) 분류분석 4가지

2. 텍스트 마이닝(영어)
- 명사 추출 & 불용어 처리
- 빈도 막대그래프

3. 시계열분석(데이터구성 : Year/month/amount)
1) 평균과 분산 일정 + 근거 & 해석
2) ARIMA + 근거 & 해석
3) 최적 모델 선택 + 근거 & 해석
4) 적합 파악

- 정상성 확인 (10점)
- ARIMA모델 3가지 제시 (10점)
- 한가지 모델을 최종 선택하고 이유를 서술 (15점)
- 최종 예측을 하고, 실제 결과와 비교 평가하고 그 평가 방법을 사용한 이유를 제시 (15점)


https://shiningyouandme.tistory.com/29

- 1번: 고객등급 예측문제
 (데이터 내용) 고객등급은 1~5등급(1등급이 높은 등급)이 있었고, 구매 금액, 구매 건수, 구매 기간 등의 feature가 주어졌습니다. 
 (의견1) 충격적인건,  SOM알고리즘이 나왔다는거..R에는 패키지가 있지만.. PYTHON은 패키지가 없어서..  이럴 수 있는것인가... ㅠㅠ
 (의견2) 파생변수를 만들라고 해서(그것도 1개가 아니라 여러개..)이부분에서도 조금 당황했어요.. 

- 2번: 영어데이터의 비정형 분석... 
  (데이터 내용) 영어...데이터(한글만 연습했는데.. 영어라니..)
  (의견1) 비정형분석은 대부분의 다른 비정형 분석 문제와 같았어요.. 
  (의견2) 역시 ADP.. 명성대로 어려운 문제를 내기보다는 예측할 수 없는 문제를 내더라구요

- 3번: 교통사고 예측(시계열)
  (데이터 내용) 이건 확실히 기억해요.. 시간과 사고건수 only two columns..
  (의견1) arima.. arima는 겉핥기가 아니라 정말 많이 써봐야 할 것 닽더라구요  ma p,q...?..기억1    도 안났음요.. 


역시나 예상 외의 부분(SOM, 다층신경망 등)이 나와 저는 다음 시험을 또 준비해야겠네요. 
​[1] Sales data. id, grade, days, count, amount
1. Eda, na처리, 시각화(10점)
2. 3개의 파생 변수 생성(10점)
3. Train, test 를 7:3으로 clustering SOM을 사용하여 예측. 그 결과를 confusion Matrix 생성(10점)
4. 랜덤포레스트, 다층 신경망 포함하여 4가지 방법으로 예측 후 그 결과를 F1제시. ROC커브 생성(10점)

[2] text분석. 영문
1. 명사만 추출. 불용어 제거(5점)
2. 빈도수 생성하여 그래프화(5점)

​[3] 계정성 시계열 분석. 년월, amount
1. 정상성 확인(10점)
2. ARIMA모델 3가지를 제시하시오(10점)
3. 한가지 모델을 최종 선택하고 그 이유를 쓰시오(15점)
4. 최종 예측을 하고, 실제 결과와 비교 평가하고 그 평가 방법을 사용한 이유를 제시하시오(15점)




================================================

[[19회]]  2020.12.13
https://evergreentags.tistory.com/17
https://mizykk.tistory.com/127?category=690990

실기 문제
1. 기계학습
1) 전처리 / 탐색적데이터분석(EDA) / 시각화
2) train-test 분리(7:3) / 분류모델 3가지 / Confusion Matrix
3) 분류모델 > 앙상블하여 예측하고 result.csv 제출하기

2. 시계열분석
1) 시계열 시각화 → 이분산성 / 정상성
2) 고정시계열 확인 & 처리
3) SARIMA 분석
4) 잔차/잡음 시각화 & 분석

그러나 SARIMA, 고정시계열은 데이터에듀 책에도 없으면서 나와서.. 조금 당황스러웠다. 데이터에듀 책만으로는 부족하고, 
다른 책이나 자료들을 참고하여 좀 더 브로드하게 준비해야하는 것 같다. 


1. 기계학습(DATA : credit데이터 - 고객이 이탈되었는지 아닌지 분류하는 문제) (총 50점)
- 독립변수로는 성별, 나이, 카드등급, 소득 등의 변수들이 있었습니다.
1-1 : 데이터 전처리 및 시각화(5점) - 연속형변수와 문자로된 범주형 변수를 처리해야합니다.
1-2 : Train과 Test를 7:3으로 나누고 분류분석 3개 실시 및 Confusion Matrix 만들기(15점)
1-3 : 위에서 실시한 분류분석 3개를 앙상블하여 Credit_test를 예측하고(credit_test.csv는 따로 주어짐) result.csv로 만들어서 제출하기(30점)
- 1-1과 1-2는 기존처럼 코드와 해석결과를 PDF로 만들어서 제출하면 되고 1-3은 CSV파일로 제출하면 됩니다.

2. 통계학습(DATA : Traffic EPS 시계열 분석 - 20년치 데이터이며 1년에 4개씩 데이터가 존재(분기별로 존재)) (총 50점)
2-1 시계열 데이터의 정규성과 이분산성을 분석하기 위해 시각화하고 설명(10점)
2-2 위에서 시계열데이터가 정규성이 아니라면, 고정시계열이 있는지 확인하고 이를 처리(15점)
2-3 SARIMA 분석을 실시, 여러 파라미터를 적용해보고 가장 성능이 좋은 것을 제시(15점)
2-4 위에서 제시한 모델의 잔차와 잡음에 대해 시각화하고 분석(10점

ADP 19회 REVIEW. sarima / marima
1. 기계학습 문제는 무난했다고 할 수 있습니다. 분류문제의 기본적인 전처리, 분류분석, 앙상블 분석 진행을 하면 되기 때문입니다. 다만 1-3은 CSV파일로 만들어서 제출해야하는데, 분석과정을 요구하지 않은 것으로 보아 성능(실제로 맞는지)만으로 평가할 것으로 예상됩니다.
2. 통계학습 문제에서는 진흥원이 '이건 몰랐지' 스킬을 또 실행했다고 할 수 있다. 사실 SARIMA라는 것을 들어본 사람은 거의 없을것입니다.. 필자도 처음 들어본 기법입니다. 주력언어는 PYTHON이지만 시계열 통계분석은 R이 유리하기 때문에 시계열은 R로 준비해갔는데 SARIMA는 처음들어봤기 때문에 패키지 찾는 곳에서 SARIMA를 직접 찾아야 했습니다. 
2-1 : 이분산성이 뭔지 몰랐는데, 등분산성과 관련있는것 같아, 등분산성과 관련하여 적었습니다. 그런데 실제로 등분산이 결여된 경우가 이분산이라고 합니다.
2-2 : 고정시계열이 뭔지 모르겠습니다.(찾아봐도 뭔지 모르겠습니다), 따라서 추세나 계절성 등을 처리하여 정상시계열을 만들라고 하는것이라고 판단하였습니다. 어처피 시계열분석을 하려면 차분을 하던지해서 정상시계열로 만들어야하기 때문이기 때문에 그 과정을 처리하는 부분이라고 생각하였습니다.
2-3 : 위에서 말했던것 처럼 R 패키지 찾는 곳에 검색을 해서 Auto.sarima라는 것을 알아내었고, 이를 활용해서 꾸역꾸역 문제를 풀었는데, 확실하지 않습니다.
2-4 : 시계열의 잔차나, 잡음 분석을 공부해가지 않아 제대로 풀지 못하였습니다.


1. 기계학습 문제 (고객 이탈 여부) (50점)
데이터 : 20개의 x와 1개의 y를 가지는 데이터로 y는 0과 1을 가짐
문제 :
EDA 및 전처리
분류모델 3개 적용 및 Coufusion Matrix 출력
3개 모델 앙상블 모형 생성 후 예측값 저장
제출 : 분석 결과 PDF 제출, 예측결과 CSV 파일 제출

2. 시계열 데이터 분석 (주가 수익률 데이터) (50점)
데이터 : 날짜 및 주가 수익률 데이터로 Time-Series 변환 전 데이터 제공
문제 :
데이터 로드, 정상성/이분산성 검증
정상성 파악 근거에 따른 고정시계열 여부 파악
SARIMA 분석 및 최적 모형 파라미터 선택
잔차 그래프 출력
제출 : 분석 결과 PDF 제출

1.기계학습 문제는 기존 출제 유형과 유사하여 큰 어려움 없이 풀었습니다. 앙상블 모형 생성, 결과값 출력이 새롭게 출제되었는데 이 부분은 조금만 연습하시면 어렵지 않게 풀 수 있으실 거라 생각합니다.
2.시계열 분석 문제는 생각지도 못한 SARIMA 문제가 출제되어 정확하게 풀지 못했습니다. ARIMA 모형에 대한 지식을 기반으로 풀이 및 해석하였으나 정확한 답이 아니었는지 점수 획득을 못하였고 탈락하게 되었습니다.
3.이번이 3번째 실기 응시였는데 매 시험마다 문제 출제 방식에 대한 의문이 가시지 않습니다. (출제자 분들은 공식 수험서에 SARIMA 모형 설명이 있는지 확인을 해보셨을까요..)
================================================
참고사항
================================================- 기존 출제 영역 : 데이터전처리, 데이터마이닝, 통계분석, 텍스트 마이닝

  1) 데이터 전처리(멍잉)
   - R 스크립트 작성 능력
   - ddply, aggregate, merge, melt/cast, sqldf 등을 이용한 처리
   - NA 결측치(imputation),이상치 처리.

  2) 데이터 마이닝 : 분류, 연관, 군집분석 수준 정도만 출제됨
   - 분류예측 : 해지여부 예측
   - 연관 : 로또번호분석
   - 군집 : 색상 스타일별 군집. 최적 k값 찾기
   - 변수 선택 능력 : nearZeroVar, findCorrelation, step(), PCA(princomp, prcomp, biplot)

  3) 통계 분석 능력 : 주로 분산분석(ANOVA),로지스틱분석 에서 출제됨
  - 교호작용, 잔차분석, 시각화 능력
  - 일원/이원분산분석  : aov(), 결측치, 이상치 고려

  4) 텍스트 마이닝 : 주로 KoNLP를 이용한 빈도 분석

예상 문제 유형 정리

1. 데이터 전처리 능력 평가 문제(멍잉)
- ddply, aggregate, merge, melt/cast, sqldf 등을 이용한 처리
- NA 결측치 처리, imputation
- 정규화(scale), 표준화

2. 변수 선택 능력 평가 문제(다변량 분석)
- nearZeroVar
- 상관관계 높은 변수 제거 (findCorrelation)
- step()
- PCA : princomp, prcomp, biplot
- 다차원분석 : cmdscale
- varImpPlot, importance

3. 통계 분석 능력
- lm : 다중공선성, 이상치 분석, 잔차분석
- 로지스틱분석,poisson,binomial
- 과대 산포 발생시 처리: quasipoisson, quasibinomial
- 교호작용
- 잔차분석
- 시각화
- 분산분석(ANOVA) : aov()

4. 최적 예측 모델 만들기
- 분류 모델 여러 개 적용 후 성능 비교 및 시각화
- svm, randomForest, ctree, …
- 훈련용/테스트용 데이터 분리, 교차검증
- caret::train(), resamples,bwplot로 비교
- 성능 평가 방법: 정확도,정밀도,민감도,Kappa,ROCR/AUC

5. 텍스트 마이닝
- 감성분석, 빈도 분석
- wordcloud
 
6. 데이터 마이닝 기타
- 군집분석 : 최적의 k값 찾기, 인사이트 도출하기
- 연관분석 : 인사이트 도출하기 

7. 시계열 분석
- ARIMA






ADP 실기 제1회

고객세분화 (45점)

가. 세분화 변수의 생성 및 선정
     –   요약 변수 및 파생 변수의 생성
     예) 카테고리별 점유율, 주 인터넷 사용 시간대, 인터넷 사용 일수, 검색 패턴, 쇼핑단계별 이용 패턴,
         주 쇼핑 시간대, 주 관심 상품 카테고리, 구매 상품 가격대
     –   EDA, 상관분석, Decision Tree 등을 통하여 적절한 세분화 변수 선정

나. <군집 분석> 및 최적 세분화 집단 생성
     –   적절한 군집 분석 기법 제시 및 분석 수행
     –   세분화 집단의 최적 개수 결정 및 기준 설명

다. 각 세분화 집단의 특성 분석, 정의 및 마케팅 인사이트
     –   각 세분화 집단에 대한 특성 분석 및 시각화
     –   특성 분석 결과를 기반으로 각 세분화 집단에 대한 마케팅 관점에서의 정의
     –   세분화 분석 결과를 토대로 타당한 마케팅 인사이트 제시


예측 (45점)
가. 세분화 집단별 예측 모형 (구매,이탈,등급 변동,우수 고객 예측 등) 개발을 위한 <종속 변수> 정의 및 <독립 변수> 선정
     –   해당 예측을 위한 타당한 종속 변수의 정의
     –   해당 예측을 위한 독립 변수의 생성 및 선정
          요약 변수 및 파생 변수의 생성, EDA/상관분석/Decision Tree 등을 통한 <적절한 변수> 선정

나. 세분화 집단별 예측 모형 개발
     –   종속 및 독립 변수의 성격에 따른 적절한 모델링 기법 제시
     –   샘플링, 파티션 등 모델링 데이터 준비 및 모형 생성
     –   적절한 평가 기준에 의한 모델 평가 및 최종 모델의 선택

다. 예측 모형 분석 및 마케팅 인사이트 제시
     –   세분화 집단별 예측 모형의 특성 분석 및 시각화
     –   세분화 집단간 예측 모형의 비교 분석
     –   예측 모형 분석 결과를 토대로 적절한 마케팅 인사이트 제시

텍스트 마이닝 (10점)

가. 포털 사이트 검색 기록을 활용한 토픽 분석
  - 한 고객이 일정 기간 동한 포털 사이트에서 입력한 전체 검색 리스트를 하나의 문서(Document)로 간주
  - 위의 문서에 기반을 두어 고객의 관심 주제를 파악하기 위한 토픽 분석을 수행

나. 토픽 분석 결과의 해석 및 마케팅에서의 활용 방안 제시
     –   도출된 토픽 리스트의 의미를 마케팅 관점에서 해석
     –   분석 결과를 고객 세분화 및 예측 등에서 활용할 수 있는 구체적인 방안 제시

[출처] 제이콥의 오픈소스 비즈니스 컨설팅

http://www.jopenbusiness.com/wordpress/?p=338



==================================================================================

ADP 실기 제2회

문제1. data munging

1) data : user_id, usage, 방문 category, 접속유지시간
- usage = userId 당 총 접속시간.
- wd_월요일 ~ wd_일요일 : user당 요일별 총 접속시간/user당 총 접속시간
- 방문 category : 총 22개 category중 user가 방문한 카테고리의 비율.
- 유저 접속일수 : user가 방문한 날짜의 수

2) 문제
user_id, usage, wd_월요일, …, wd_일요일, 방문카테고리 비율, 유저 접속일수의 순서로 데이터를 출력.  단, wd_월요일 ~ 방문 카테고리 비율은 소수점 3자리로 맞추어서 출력하라
(aggregate, plyr, reshape 패키지를 연계해서 잘 다루어야 풀 수 있는 문제)

문제2. data mining
1)data : custid, churn(해지 또는 해약), product_Id
custid, churn, product_001,,,,

2) 문제
- 3가지 이상의 churn 예측모델링을 하고, 각각의 성능을 비교하여, 최적의 모델을 선정하고, 이를 시각화하시오.
(의사결정나무, svm, logistic regression, randomForest등을 예시)
- 문제 : 고객들은 churn고객과 아닌 고객으로 구분한 후, 각각을 대상으로 <연관분석>을 실시하고, 그 결과를 가지고 상기 고객군의 특성을 비교하라.

문제 3. 통계분석
1)data
일일평균 교통량, 도시인구, 도로의 차선수
도로의 종류, 트럭의 종류, 도시/농촌여부
2) 문제
- 주어진 변수를 사용하여 <변수 선택과정>을 포함하여, 예측 모델을 만들어라.
- 도출된 모델의 <잔차 분석>을 통해 교정을 해서 최적의 모델을 찾고, 최종 모델의 결과로 나온 각 파라미터의 의미를  해석하시오.

문제 4. 텍스트 마이닝
1) data
제주관광명소 txt 파일
블로그 문서 txt 파일
2) 문제
- 데이터로딩에 문제가 있으면 적절한 형태로 데이터를 조정하고, TITLE과 CONTENT에 “제주”나 “관광”이 들어가 문서를 찾으시오.
-문서에서 명사를 추출하고, 추출된 단어에서 제주관광명소가 아닌 곳을 필터링하시오.
- 단어가 적용된 문서를 기준으로 빈발단어 10개를 찾고, 이를 그래프로 만드시오.

[출처]장운호님
https://docs.google.com/presentation/d/1nfjNz95C3Xb6BFjXQyWktNWXAcZqpYzOn-DDQIXW4JA/edit#slide=id.g435dedd7b_10

==================================================================================

ADP 실기 제3회

멍잉, 데이터마이닝, 텍스트마이닝, 통계분석, 인사이트 도출

1) 멍잉
멍잉 부분에선 고객 자료주고 고객별 총 구매액 총 거래빈도 최근 구매일자 등을 파생변수로 만들고 0-1로 표준화하여 스코어를 구하고 정렬해서 상위 20명 나타내기

2) 데이터마이닝
로또 번호를 가지고 연관분석 실시

3) 텍스트마이닝
뉴스 자료 에서 뉴스 키워드 분석
주간 이슈, 이슈별 빈도 등등

4) 통계분석
당뇨병 환자 정보 주고 나이와 성별이 사망과 관계 있는지 비율과 그래프 이용해 분석하고 상호작용 판단하는거와 로지스틱 모형을 범주형과 연속형 나이로 각각 만들고 어느 모형이 나은지 그 후에 상호작용항 적용하여 교호작용 판단하고 최종모형 결정 후 최종 파라미터 해석

5) 인사이트 도출
신규 스타일 컬러가 주어졌을 때 어떻게 유사 클래스 정하는지 ?
군집분석이 어떻게 그룹 지정하는지 ?
실제와 예측 트랜드 유사도 확인 방법과 패턴 불일치 시 고려할 외부요인 등
신규만 판매 늘고 유사 클래스는 감소할 경우 전략 등을 적기

[출처] ADP 3회 실기 후기 (데이터 분석 활용 까페) |작성자 해현용님 
http://cafe.naver.com/datageeks/2142


Q) timestamp 컬럼에 들어있는 1543590900 값을 어떻게 변환할 것인가?
시험 종료 후 네이버 카페의 후기들을 보다보니 다른 능력자 분께서 1543590900는 "
UNIX시스템에서 사용하는 timestamp 형식이다
" 라는 힌트를 주셨고, 추가로 찾아보니 아래와 같이 timestamp를 변환해 볼 수 있는 사이트도 있었습니다.
library(lubridate)
as_datetime(1543590900)
# [1] "2018-11-30 15:15:00 UTC"

아래는 샘플 데이터를 생성하여 UNIX 날짜 형식 변환 후, 연도 및 시간을 분리해 내는 예제 입니다.

먼저 UNIX 날짜 형식을 가진 dt변수를 생성해 보겠습니다.

dt <- data.frame(timestamp = c(1543590900, 1543591000, 1543591100),
                 A = c(4, 5, 6),
                 B = c(10, 30, 70),
                 C = c(6, 1, 7))

dt
#    timestamp A  B C
# 1 1543590900 4 10 6
# 2 1543591000 5 30 1
# 3 1543591100 6 70 7

dt$datetime <- as_datetime(dt$timestamp)
dt$yyyymmdd <- format(dt$datetime, "%Y%m%d")
dt$hhmm <- format(dt$datetime, "%H%S")

#    timestamp A  B C            datetime yyyymmdd hhmm
# 1 1543590900 4 10 6 2018-11-30 15:15:00 20181130 1500
# 2 1543591000 5 30 1 2018-11-30 15:16:40 20181130 1540
# 3 1543591100 6 70 7 2018-11-30 15:18:20 20181130 1520






텍스트 마이닝(Text Mining)
[1회, 2014-06-24]
포털 검색 리스트 활용 토픽 분석 : 사용자 로그 문서화, 토픽 분석 결과 해석
[2회, 2014-10-18]
제주관광명소 분석 : 명소 txt 및 블로그 데이터, 필터링, 빈발단어 분석
[3회, 2015-01-24]
뉴스 키워드 분석 : 주간 이슈, 이슈별 빈도 등

[6회, 2016-04-30]
여행지 분석 : 가평/가야/남이성 등, 봄 관련 문서 추출, 명사 추출 및 빈도 분석, 상위 10개 시각화
[7회, 2016-10-29]
갤럭시/아이폰 기사 감성분석
[8회, 2017-04-29]
Tvprogram / Tvprogrambuz
Tvprogram 파일에 있는 단어들을 사전에 추가하는 것
Tvprogrambuz 파일을 읽고 데이터 전처리
월별/프로그램 별 나온 단어 분석
월별 프로그램 비율 확인
[11회, 2018-10-27]
영화 리뷰 분석 : 형용사 추출 및 감성 분석
[12회, 2019-03-30]
뉴스기사 로딩 및 제공된 긍정/부정 어휘를 통한 감성분석
위에서 구한 긍정부정 score를 통해 N개의 그룹으로 클러스터링

ADP 실기시험 13회부터 17회 사이에는 텍스트 마이닝 과목이 출제되지 않았습니다.

[18회, 2020-09-19]
텍스트 마이닝 (문제 상세 파악 안됨)
명사 추출 & 불용어 처리
빈도 막대 그래프






데이터 마이닝 & 기계 학습 (Data Mining & Machine Learning)
[1회, 2014-06-24]
고객 세분화 : 세분화 변수 생성 및 선정, 군집분석, 집단별 특성 분석 및 인사이트 도출
[2회, 2014-10-18]
해지 예측(churn) : 3가지 이상 모델 도출 및 비교/선정
[3회, 2015-01-24]
로또 번호 연관분석

[6회, 2016-04-30]
sales데이터 분석 : 고객/제품별 처리, 피어슨상관분석, 특정 유저와 유사한 15명 도출, 특정 유저당 5개 상품 추천
[7회, 2016-10-29]
홈쇼핑 구매데이터 분석 : 구매내역 데이터 제공, EDA, F/E, Classification
[8회, 2017-04-29]
백화점 사용패턴 분석 (하단 데이터 예시)
Date(0000-00-00 00:00 형태)
customer id
product (한글 백화점 상품명)
price (가격)
Text
파생 변수 생성 및 EDA
군집분석 및 적절한 집단 갯수 설정
세그멘테이션 별 의미 파악 (집단 라벨링)
세그멘테이션 별 마케팅 인사이트 도출
[11회, 2018-10-27]
타이타닉 생존 예측 : 생존률 예측(과정 평가x, 적중률로만 평가)
[12회, 2019-03-30]
고객 구매데이터(transaction data) 분석
이후 1개월 간 실제 고객이 구매할 것으로 예정되는 지점 추천
5개 지점 추천 후 하나라도 맞으면 맞는 것으로 간주
적중률 66.7% 이상인 경우만 채점, 그 이하인 경우 0점으로 간주
[13회, 2019-06-29]
마케팅 데이터 분석
시각화 및 전처리를 통해 세분화 작업에 필요한 변수 추출 및 파생변수 생성
군집분석을 통한 적절한 군집 수 파악 및 군집 정의
마케팅 인사이트 도출
문항별로 "시각화 필수" 조건이 있었음
[14회, 2019-09-07]
Califonia 주택 가격 예측 : EDA, NA처리, F/E, 집 가격 예측/검증
[15회, 2019-12-14]
제조 생산 데이터 분석
데이터 탐색 : EDA
데이터 전처리 : 변수 선택(VIF), 파생변수 생성, 데이터 분할(train/validation/test(20%))
로지스틱 분석 : 분류1 을 판단 하는 모델 생성 (종속변수는 총 7개 분류, 분류1 외의 값은 0으로 치환), confusion matrix 해석
로지스틱 분석 외 3개 이상 분류 모델 생성 및 결과 해석 : SVM 필수 포함, Precision/Sensitivity 결과 출력
위 모델 중 최고 모델을 선택하여 최적의 군집 개수를 선택하고 클러스터링 수행 : F-1 Score 출력

[17회, 2020-06-21]
집값 예측 문제
EDA
모델 생성
데이터 분할
교호작용을 고려한 다중 선형 회귀 수행
3가지 분류 모델 생성 및 비교, 좋은 모델 선택
[18회, 2020-09-19]
고객 등급 예측 모형
EDA & 결측값 채우기
파생변수 3개 생성 & 이유 작성
Train-Test 분할(7:3) / SOM 군집분석 / 정오분류표
분류분석 4가지
[19회, 2020-12-13]
고객 이탈 에측
20개의 x와 1개의 y를 가지는 데이터로 y는 0과 1을 가짐
EDA 및 전처리
분류모델 3개 적용 및 Coufusion Matrix 출력
3개 모델 앙상블 모형 생성 후 예측값 저장







통계 분석 (Statistical Analysis)
[1회, 2014-06-24]
=>예측 : 세분화 집단별 예측 모형 정의, 예측 모형 개발, 인사이트 제시

[2회, 2014-10-18]
=>예측 : 변수 선택과정 포함, 모델 잔차분석 및 교정, 최적모델 도출 및 파라미터 해석

[3회, 2015-01-24]
=>당뇨병 환자 나이/성별 별 사망 여부 분석 : 비율/그래프, 상호작용 판단, 범주형/연속형 분석 결과 비교, 교호작용 판단 후 최종 파라미터 해석

[6회, 2016-04-30]
=>risk분석(성별/인종/나이/활동성/risk) : 변수간 차이 분석, 교호작용 분석

[7회, 2016-10-29]
=>집단간 의료비 지출 차이 분석 : t-test, ANOVA, 회귀분석

[8회, 2017-04-29]
=>폐활량(Fev) 예측 문제 (나이, 키, 성별, 흡연 유무)
1)EDA 및 상관관계 분석
2)적절한 회귀모형 선택
3)회귀모형 해석 (변수 별 증가에 따른)
4)평균 키, 나이 (여자, 흡연자) 일경우 폐활량 예측

[11회, 2018-10-27]
=>출산률 분석 : 독립/반응변수 관계를 회귀분석으로 정의 및 결과 해석

[12회, 2019-03-30]
=>회귀 분석 문제
1)변수 시각화(변수간 상관관계, 변수별 이상치 파악)
2)회귀모형 적합과 유의성 검정
3)회귀 계수에 대한 standard error가 가지는 의미
4)회귀분석에서 잔차 분석 및 시각화
5)회귀분석에서 영향력 관측치와 그 영향 분석

[13회, 2019-06-29]
=>거래내역 데이터를 통해 부정사용여부 탐지 모델 개발
1)타겟변수 불균형 문제 처리 : resampling, undersampling, oversampling(SMOTE) 특징(장단점) 서술 및 적용(패키지 활용가능)
2)간단한 시각화
3)불균형 문제가 해결된 resampled data로 binary classification model 생성
4)Confusion matrix와 AUC 등 다양한 성능 지표를 적용하여 결과 해석
분류문제에서 어떤 것을 주로 봐야하는지 판단 필요

[14회, 2019-09-07]
=>로지스틱 회귀 분석 문제
1)주성분 분석(PCA)
2)주성분별 의미 해석
3)주성분을을 독립변수로 하여 다항 로지스틱 회귀분석 수행
4)Confusion Matrix 해석

[15회, 2019-12-14]
=>데이터 처리 및 통계 분석
- timestamp 처리 / date 기준 데이터 병합
- hh:mm, A/B/C/D/E, 전력 사용량 데이터 
- yyyymmdd, 평균 기온
1)아래 형태의 데이터 생성 : 3개 데이터를 date기준으로 병합 필요
-> yyyy-mm / A / B / C / D / E / 사용량
2)요일 변수 생성 및 A/B/C/D/E별 평균 사용량 출력, 그래프 출력
3)요일간 사용량 분석을 수행하고 가장 차이가 있는 요일 도출
4)(문제가 생각이 안나네요.. ㅠㅠ)

[17회, 2020-06-21]
=>통계 분석 (설문데이터 분석) -> (사전에 역문항들에 대한 처리 필요)
1)그룹별 평균, 표준편차, 왜도, 첨도 산출
2)(문제가 기억이 나질 않네요;)
3)(문제가 기억이 나질 않네요;)
4)(문제가 기억이 나질 않네요;)

=>시각화 및 시계열 분석 (코로나 데이터) (20점)
1)전체 인구대비 누적 사망률이 가장 높은 5개 국가 추출 후, 국가별 일일확진자, 누적확진자, 일일사망자, 누적사망자 시계열 그래프 출력
2)위험지수 생성 및 해석
3)시계열 분석 및 예측 모델 생성

[18회, 2020-09-19]
시계열 분석
- 데이터 구성 : Year / month / amount
1)정상성 확인 : 평균과 분산 일정 + 근거 & 해석 (10점)
2)ARIMA 모델 3가지 제시 (10점)
3)최적 모델 선택 및 근거 서술 (15점)
4)최종 예측 후 실제 결과와 비교 평가, 평가 방법 사용 이유 제시 (15점)

[19회, 2020-12-13]
시계열 분석
- 날짜 및 주가 수익률 데이터 (Time-Series 변환 전 데이터 제공)
1)데이터 로드, 정상성/이분산성 검증
2)정상성 파악 근거에 따른 고정시계열 여부 파악
3)SARIMA 분석 및 최적 모형 파라미터 선택
4)잔차 그래프 출력

https://didalsgur.tistory.com/87








1. 머신러닝
  - 모델 결정
  - 주요 메트릭 정리 : 특히 AUC
  - validation에 대한 로직

2. 회귀
  - 다중공선성 정리
  - 변수선택
  - 회귀 진단

3. 텍스트 마이닝
  - fread( ) 함수를 이용해 readLines의 효과내기 : f <- fread("Batting.csv", sep= "?", header = FALSE)[[1L]]  # 파일에 없는 구분자를 사용
                                                                차라리 sep="\n" 라고 두는 것이 가능하면 이렇게도 괜찮을 듯?
                                         [[1L]] 을 추가하면 캐릭터 타입으로 변경됨. (왜인지는 모르겠음)
    ㆍhttps://stackoverflow.com/questions/32920031/how-to-use-fread-as-readlines-without-auto-column-detection
  - 한글 불러올 때 문제점
  - 워드클라우드
  - 명사/형용사
  - 정규표현식(쓸데 없는 표현 제거 정리)
  - 감성분석 
    ㆍ감성사전 등록
    ㆍ분석 대상 전처리(불용어 처리)
    ㆍ본문 단어와 긍정/부정 감성 사전 단어를 매칭
    ㆍ이를 계산하여 결론을 산출하는 로직 구현s


------------------------------------------------------------------------------

ADP실기를 위해 준비한 것들
1. 회귀분석(회귀분석 평가 및 statsmodel을 활용한 회귀 통계 분석, 잔차의 정규성/독립성/등분산성 확인, 변수선택법 등)

2. 분류분석(RandomForest, gradientboost, 로지스틱회귀, 인공신경망 등의 각종 분류방법과 voting 등 각종 앙상블 방법 및 confusion matrix등 분류분석 평가 등)

3. 교차분석, 분산분석(ANOVA), 상관분석

4. 군집분석(KNN, SOM. EM알고리즘 활용 등)

5. 연관분석(Aprori 알고리즘)

6. 표본추출과 T검정

7. 탐색적 요인분석(FA), 주성분분석(PCA), LEE 등 다양한 차원축소법

8. 시계열분석(ARIMA)

9. 타겟변수 불균형 처리 해결(언더샘플링, 오버샘플링)

10. 특징선택(분산, 단일변수 선택, 특성중요도 계산)

11. Datetime, Random 등 기타 pandas 활용 함수들



---
책을 보고 공부하지는 않았구요 계절성을 가지고 있는 시계열 데이터를 예측하는 시계열 모형에 대해서 찾아보시면 아마 필연적으로 보게되는게 sarima입니다. 더 나아가서는 하나의 계절성만을 가정하는 sarima를 넘어서 tbats나 fbprophet등의 모델도 있고요 이번에 파이썬 라이브러리에 fbprophet이 있길래 혹시나했는데 나오지는 않았지만 한 번 찾아보시면 자료는 많을거에요 구글에
SARIMA는 파라메터가 6개인가 그럴거에요
Arima는 pdq 3개
---

고객세분화 (45점)


가. 세분화 변수의 생성 및 선정
     -   요약 변수 및 파생 변수의 생성
         예) 카테고리별 점유율, 주 인터넷 사용 시간대, 인터넷 사용 일수, 검색 패턴, 쇼핑단계별 이용 패턴,
              주 쇼핑 시간대, 주 관심 상품 카테고리, 구매 상품 가격대
     -   EDA, 상관분석, Decision Tree 등을 통하여 적절한 세분화 변수 선정

나. 군집 분석 및 최적 세분화 집단 생성
     -   적절한 군집 분석 기법 제시 및 분석 수행
     -   세분화 집단의 최적 개수 결정 및 기준 설명

다. 각 세분화 집단의 특성 분석, 정의 및 마케팅 인사이트
     -   각 세분화 집단에 대한 특성 분석 및 시각화
     -   특성 분석 결과를 기반으로 각 세분화 집단에 대한 마케팅 관점에서의 정의
     -   세분화 분석 결과를 토대로 타당한 마케팅 인사이트 제시




예측 (45점)


가. 세분화 집단별 예측 모형 (구매, 이탈, 등급 변동, 우수 고객 예측 등) 개발을 위한 종속 변수 정의 및 독립 변수 선정
     -   해당 예측을 위한 타당한 종속 변수의 정의
     -   해당 예측을 위한 독립 변수의 생성 및 선정
          요약 변수 및 파생 변수의 생성, EDA/상관분석/Decision Tree 등을 통한 적절한 변수 선정

나. 세분화 집단별 예측 모형 개발
     -   종속 및 독립 변수의 성격에 따른 적절한 모델링 기법 제시
     -   샘플링, 파티션 등 모델링 데이터 준비 및 모형 생성
     -   적절한 평가 기준에 의한 모델 평가 및 최종 모델의 선택

다. 예측 모형 분석 및 마케팅 인사이트 제시
     -   세분화 집단별 예측 모형의 특성 분석 및 시각화
     -   세분화 집단간 예측 모형의 비교 분석
     -   예측 모형 분석 결과를 토대로 적절한 마케팅 인사이트 제시



텍스트 마이닝 (10점)


가. 포털 사이트 검색 기록을 활용한 토픽 분석
     -   한 고객이 일정 기간 동한 포털 사이트에서 입력한 전체 검색 리스트를 하나의 문서(Document)로 간주
     -   위의 문서에 기반을 두어 고객의 관심 주제를 파악하기 위한 토픽 분석을 수행

나. 토픽 분석 결과의 해석 및 마케팅에서의 활용 방안 제시
     -   도출된 토픽 리스트의 의미를 마케팅 관점에서 해석
     -   분석 결과를 고객 세분화 및 예측 등에서 활용할 수 있는 구체적인 방안 제시
